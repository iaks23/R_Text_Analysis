z15g_avg <- colMeans(z15_garbage[ , 4, drop = FALSE])
z15g_total <- colSums(z15_garbage[ , 4, drop = FALSE])
zone15.collection <- data.frame("Zone 15",z15r_avg,z15g_avg,"Y")
names(zone15.collection) <-c("zones","recycle_avg","garbage_avg","zero_val")
zone_df <- rbind(zone_df,zone15.collection)
zone_df
z16 <- working_dset[(working_dset$zone==16),]
table(z16$weight) # 8 0-observations
z16_garbage <- z16[(z16$bin_type=='G-0140') | (z16$bin_type=='GA-0140') | (z16$bin_type=='G-0240'),]
z16_recycle <- z16[(z16$bin_type=='R-0240') | (z16$bin_type=='RA-0240'),]
z16r_total <- colSums(z16_recycle[ , 4, drop = FALSE])
z16r_avg <- colMeans(z16_recycle[ , 4, drop = FALSE])
z16g_avg <- colMeans(z16_garbage[ , 4, drop = FALSE])
z16g_total <- colSums(z16_garbage[ , 4, drop = FALSE])
zone16.collection <- data.frame("Zone 16",z16r_avg,z16g_avg,"Y")
names(zone16.collection) <-c("zones","recycle_avg","garbage_avg","zero_val")
zone_df <- rbind(zone_df,zone16.collection)
zone_df
z17 <- working_dset[(working_dset$zone==17),]
table(z17$weight) # 7 0-observations
z17_garbage <- z17[(z17$bin_type=='G-0140') | (z17$bin_type=='GA-0140') | (z17$bin_type=='G-0240'),]
z17_recycle <- z17[(z17$bin_type=='R-0240') | (z17$bin_type=='RA-0240'),]
z17r_total <- colSums(z17_recycle[ , 4, drop = FALSE])
z17r_avg <- colMeans(z17_recycle[ , 4, drop = FALSE])
z17g_avg <- colMeans(z17_garbage[ , 4, drop = FALSE])
z17g_total <- colSums(z17_garbage[ , 4, drop = FALSE])
zone17.collection <- data.frame("Zone 17",z17r_avg,z17g_avg,"Y")
names(zone17.collection) <-c("zones","recycle_avg","garbage_avg","zero_val")
zone_df <- rbind(zone_df,zone17.collection)
zone_df
z18 <- working_dset[(working_dset$zone==18),]
table(z18$weight) # 1 0-observations
z18_garbage <- z18[(z18$bin_type=='G-0140') | (z18$bin_type=='GA-0140') | (z18$bin_type=='G-0240'),]
z18_recycle <- z18[(z18$bin_type=='R-0240') | (z18$bin_type=='RA-0240'),]
z18r_total <- colSums(z18_recycle[ , 4, drop = FALSE])
z18r_avg <- colMeans(z18_recycle[ , 4, drop = FALSE])
z18g_avg <- colMeans(z18_garbage[ , 4, drop = FALSE])
z18g_total <- colSums(z18_garbage[ , 4, drop = FALSE])
zone18.collection <- data.frame("Zone 18",z18r_avg,z18g_avg,"Y")
names(zone18.collection) <-c("zones","recycle_avg","garbage_avg","zero_val")
zone_df <- rbind(zone_df,zone18.collection)
zone_df
z19 <- working_dset[(working_dset$zone==19),]
table(z19$weight) # 8 0-observations
z19_garbage <- z19[(z19$bin_type=='G-0140') | (z19$bin_type=='GA-0140') | (z19$bin_type=='G-0240'),]
z19_recycle <- z19[(z19$bin_type=='R-0240') | (z19$bin_type=='RA-0240'),]
z19r_total <- colSums(z19_recycle[ , 4, drop = FALSE])
z19r_avg <- colMeans(z19_recycle[ , 4, drop = FALSE])
z19g_avg <- colMeans(z19_garbage[ , 4, drop = FALSE])
z19g_total <- colSums(z19_garbage[ , 4, drop = FALSE])
zone19.collection <- data.frame("Zone 19",z19r_avg,z19g_avg,"N")
names(zone19.collection) <-c("zones","recycle_avg","garbage_avg","zero_val")
zone_df <- rbind(zone_df,zone19.collection)
zone_df
z20 <- working_dset[(working_dset$zone==20),]
table(z20$weight)
z20 <- working_dset[(working_dset$zone==20),]
table(z20$weight)
z20_garbage <- z20[(z20$bin_type=='G-0140') | (z20$bin_type=='GA-0140') | (z20$bin_type=='G-0240'),]
z20_recycle <- z20[(z20$bin_type=='R-0240') | (z20$bin_type=='RA-0240'),]
z20r_total <- colSums(z20_recycle[ , 4, drop = FALSE])
z20r_avg <- colMeans(z20_recycle[ , 4, drop = FALSE])
z20g_avg <- colMeans(z20_garbage[ , 4, drop = FALSE])
z20g_total <- colSums(z20_garbage[ , 4, drop = FALSE])
zone20.collection <- data.frame("Zone 20",z20r_avg,z20g_avg,"N")
names(zone20.collection) <-c("zones","recycle_avg","garbage_avg","zero_val")
zone_df <- rbind(zone_df,zone20.collection)
zone_df
z21 <- working_dset[(working_dset$zone==21),]
table(z21$weight)
table(working_dset$zone)
table(z21$weight)
z21_garbage <- z21[(z21$bin_type=='G-0140') | (z21$bin_type=='GA-0140') | (z21$bin_type=='G-0240'),]
z21_recycle <- z21[(z21$bin_type=='R-0240') | (z21$bin_type=='RA-0240'),]
z21r_total <- colSums(z21_recycle[ , 4, drop = FALSE])
z21r_avg <- colMeans(z21_recycle[ , 4, drop = FALSE])
z21g_avg <- colMeans(z21_garbage[ , 4, drop = FALSE])
z21g_total <- colSums(z21_garbage[ , 4, drop = FALSE])
zone21.collection <- data.frame("Zone 21",z21r_avg,z21g_avg,"Y")
names(zone21.collection) <-c("zones","recycle_avg","garbage_avg","zero_val")
zone_df <- rbind(zone_df,zone21.collection)
zone_df
# ZONE 22
z22 <- working_dset[(working_dset$zone==22),]
table(z22$weight) # 2 0-observations
z22_garbage <- z22[(z22$bin_type=='G-0140') | (z22$bin_type=='GA-0140') | (z22$bin_type=='G-0240'),]
z22_recycle <- z22[(z22$bin_type=='R-0240') | (z22$bin_type=='RA-0240'),]
z22r_total <- colSums(z22_recycle[ , 4, drop = FALSE])
z22r_avg <- colMeans(z22_recycle[ , 4, drop = FALSE])
z22g_avg <- colMeans(z22_garbage[ , 4, drop = FALSE])
z22g_total <- colSums(z22_garbage[ , 4, drop = FALSE])
zone22.collection <- data.frame("Zone 22",z22r_avg,z22g_avg,"Y")
names(zone22.collection) <-c("zones","recycle_avg","garbage_avg","zero_val")
zone_df <- rbind(zone_df,zone22.collection)
zone_df
z23 <- working_dset[(working_dset$zone==23),]
table(z23$weight)
View(zone_df)
rm(list = ls())
################ Loading Packages ################
library(here)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(caret)
library(Rcpp)
library(Amelia)
library(gcookbook)
#################Finding Missing Data######################
is.null(mood_data)
missmap(mood_data, main = "Missing V/S Observed")
#################Data Cleaning######################
summary(mood_data)
mood_data$Date <- as.Date(mood_data$Date, format = "%d/%m/%Y")
mood_data <- read.csv(file = here("/Users/akshayaparthasarathy/Desktop/DSI/Mood-O-Meter.csv"))
head(mood_data)
#################Finding Missing Data######################
is.null(mood_data)
missmap(mood_data, main = "Missing V/S Observed")
#################Data Cleaning######################
summary(mood_data)
mood_data$Date <- as.Date(mood_data$Date, format = "%d/%m/%Y")
library(RColorBrewer)
library(gridExtra)
col2 = "#d64b11"
col1 = "#e69370"
morn <- ggplot(mood_clean, aes(x=Date, y=Aliases))+
geom_tile(aes(fill=Morning))+
scale_fill_gradient(low = col1, high = col2)+
scale_x_date(date_breaks = "1 week", date_labels="%d/%m (%A)")+
ggtitle("Progression of Mood in the Mornings")+
labs(x="Date", y="Aliases")+
coord_fixed()
noon <- ggplot(mood_clean, aes(x=Date, y=Aliases))+
geom_tile(aes(fill=Noon))+
scale_fill_gradient(low = col1, high = col2)+
scale_x_date(date_breaks = "1 week", date_labels="%d/%m (%A)")+
ggtitle("Progression of Mood during Afternoon")+
labs(x="Date", y="Aliases")+
coord_fixed()
eve <- ggplot(mood_clean, aes(x=Date, y=Aliases))+
geom_tile(aes(fill=Evening))+
scale_fill_gradient(low = col1, high = col2)+
scale_x_date(date_breaks = "1 week", date_labels="%d/%m (%A)")+
ggtitle("Progression of Mood in the Evenings")+
labs(x="Date", y="Aliases")+
coord_fixed()
night <- ggplot(mood_clean, aes(x=Date, y=Aliases))+
geom_tile(aes(fill=Evening))+
scale_fill_gradient(low = col1, high = col2)+
scale_x_date(date_breaks = "1 week", date_labels="%d/%m (%A)")+
ggtitle("Progression of Mood in the Night")+
labs(x="Date", y="Aliases")+
coord_fixed()
grid.arrange(morn, noon, eve, night, nrow = 4)
#################Data Cleaning######################
summary(mood_data)
mood_data$Date <- as.Date(mood_data$Date, format = "%d/%m/%Y")
mood_clean <- na.omit(mood_data)
head(mood_clean$Morning)
summary(mood_clean$Date)
library(RColorBrewer)
library(gridExtra)
col2 = "#d64b11"
col1 = "#e69370"
morn <- ggplot(mood_clean, aes(x=Date, y=Aliases))+
geom_tile(aes(fill=Morning))+
scale_fill_gradient(low = col1, high = col2)+
scale_x_date(date_breaks = "1 week", date_labels="%d/%m (%A)")+
ggtitle("Progression of Mood in the Mornings")+
labs(x="Date", y="Aliases")+
coord_fixed()
noon <- ggplot(mood_clean, aes(x=Date, y=Aliases))+
geom_tile(aes(fill=Noon))+
scale_fill_gradient(low = col1, high = col2)+
scale_x_date(date_breaks = "1 week", date_labels="%d/%m (%A)")+
ggtitle("Progression of Mood during Afternoon")+
labs(x="Date", y="Aliases")+
coord_fixed()
eve <- ggplot(mood_clean, aes(x=Date, y=Aliases))+
geom_tile(aes(fill=Evening))+
scale_fill_gradient(low = col1, high = col2)+
scale_x_date(date_breaks = "1 week", date_labels="%d/%m (%A)")+
ggtitle("Progression of Mood in the Evenings")+
labs(x="Date", y="Aliases")+
coord_fixed()
night <- ggplot(mood_clean, aes(x=Date, y=Aliases))+
geom_tile(aes(fill=Evening))+
scale_fill_gradient(low = col1, high = col2)+
scale_x_date(date_breaks = "1 week", date_labels="%d/%m (%A)")+
ggtitle("Progression of Mood in the Night")+
labs(x="Date", y="Aliases")+
coord_fixed()
grid.arrange(morn, noon, eve, night, nrow = 4)
head(mood_clean)
example <- mood_clean[(mood_clean$SNo.==106)|(mood_clean$SNo.==133)|(mood_clean$SNo.==130),]
example
example$Aliases
example <- mood_clean[(mood_clean$SNo.==107)|(mood_clean$SNo.==134)|(mood_clean$SNo.==131),]
example$Aliases
example
View(example)
morn
noon
eve
night
rm(list = ls())
################ Loading Packages ################
library(here)
library(tidyverse)
library(ggplot2)
library(janitor)
library(dplyr)
library(RColorBrewer)
library(tm) # for text mining
library(SnowballC) # for text stemming
library(wordcloud) # word-cloud generator
library(syuzhet) # for sentiment analysis
library(gridExtra)
#Loading the dataset
getwd()
setwd("/Users/akshayaparthasarathy/Desktop/WORK/R_Text_Analysis/Dataset")
dset <- read_csv("regrets.csv")
View(dset)
head(dset)
summary(dset)
dset <- clean_names(dset)
summary(dset)
dset_young <- dset %>% filter(age >= 5 & age < 25)
dset_mid <- dset %>% filter(age >= 25 & age < 50)
dset_old <- dset %>% filter(age>=50 & age < 76)
View(dset_young)
#Working with required columns for text analysis
dset_analysis <- dplyr::select(dset_young, c('regret'))
#Converting Dataframe into a Text Corpse
TextDoc <- Corpus(VectorSource(dset_analysis))
Summary(dset_analysis)
head(dset_analysis)
#Replacing : - and , with space
toSpace <- content_transformer(function(x , pattern) gsub(pattern," ", x))
TextDoc <- tm_map(TextDoc, toSpace, ",")
TextDoc <- tm_map(TextDoc, toSpace, "\"")
TextDoc <- tm_map(TextDoc, toSpace, "-")
#Convert all text to lowercase
TextDoc <- tm_map(TextDoc, content_transformer(tolower))
TextDoc <- tm_map(TextDoc, removePunctuation)
#Eliminate extra white spaces
TextDoc <- tm_map(TextDoc, stripWhitespace)
#Building the term-document matric
TextDoc_tdm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_tdm)
#Sort by decreasing value of Freq
dtm_v <- sort(rowSums(dtm_m), decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v), freq=dtm_v)
#Display top 10
head(dtm_d, 10)
#generate word cloud
set.seed(1234)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 5,
max.words=100, random.order=FALSE, rot.per=0.40,
colors=brewer.pal(8, "Dark2"))
#generate word cloud
set.seed(1234)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.40,
colors=brewer.pal(8, "Dark2"))
#Alternatively, we can split based on gender
dset_f <- dset %>% filter(gender=="F")
dset_m <- dset %>% filter(gender=="M")
#Working with required columns for text analysis
dset_analysis <- dplyr::select(dset_f, c('regret'))
#Converting Dataframe into a Text Corpse
TextDoc <- Corpus(VectorSource(dset_analysis))
head(dset_analysis)
#Replacing : - and , with space
toSpace <- content_transformer(function(x , pattern) gsub(pattern," ", x))
TextDoc <- tm_map(TextDoc, toSpace, ",")
TextDoc <- tm_map(TextDoc, toSpace, "\"")
TextDoc <- tm_map(TextDoc, toSpace, "-")
#Convert all text to lowercase
TextDoc <- tm_map(TextDoc, content_transformer(tolower))
#Remove punctuations
TextDoc <- tm_map(TextDoc, removePunctuation)
#Eliminate extra white spaces
TextDoc <- tm_map(TextDoc, stripWhitespace)
#Building the term-document matric
TextDoc_tdm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_tdm)
#Sort by decreasing value of Freq
dtm_v <- sort(rowSums(dtm_m), decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v), freq=dtm_v)
#Display top 10
head(dtm_d, 10)
TextDoc <- tm_map(TextDoc, removeWords, c("that","the","and"))
#Building the term-document matric
TextDoc_tdm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_tdm)
#Sort by decreasing value of Freq
dtm_v <- sort(rowSums(dtm_m), decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v), freq=dtm_v)
#Display top 10
head(dtm_d, 10)
#generate word cloud
set.seed(1234)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.40,
colors=brewer.pal(8, "Dark2"))
TextDoc <- tm_map(TextDoc, removeWords, c("that","the","and","not"))
#Building the term-document matric
TextDoc_tdm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_tdm)
#Sort by decreasing value of Freq
dtm_v <- sort(rowSums(dtm_m), decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v), freq=dtm_v)
#Display top 10
head(dtm_d, 10)
#generate word cloud
set.seed(1234)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.40,
colors=brewer.pal(8, "Dark2"))
TextDoc <- tm_map(TextDoc, removeWords, c("that","the","and","not","when","was","with"))
#Building the term-document matric
TextDoc_tdm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_tdm)
#Sort by decreasing value of Freq
dtm_v <- sort(rowSums(dtm_m), decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v), freq=dtm_v)
#Display top 10
head(dtm_d, 10)
#generate word cloud
set.seed(1234)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.40,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 2,
max.words=100, random.order=FALSE, rot.per=0.40,
colors=brewer.pal(8, "Dark2"))
#generate word cloud
set.seed(1234)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 3,
max.words=100, random.order=FALSE, rot.per=0.40,
colors=brewer.pal(8, "Dark2"))
#Working with required columns for text analysis
dset_analysis <- dplyr::select(dset_m, c('regret'))
#Converting Dataframe into a Text Corpse
TextDoc <- Corpus(VectorSource(dset_analysis))
head(dset_analysis)
#Replacing : - and , with space
toSpace <- content_transformer(function(x , pattern) gsub(pattern," ", x))
TextDoc <- tm_map(TextDoc, toSpace, ",")
TextDoc <- tm_map(TextDoc, toSpace, "\"")
TextDoc <- tm_map(TextDoc, toSpace, "-")
#Convert all text to lowercase
TextDoc <- tm_map(TextDoc, content_transformer(tolower))
#Remove punctuations
TextDoc <- tm_map(TextDoc, removePunctuation)
#Eliminate extra white spaces
TextDoc <- tm_map(TextDoc, stripWhitespace)
TextDoc <- tm_map(TextDoc, removeWords, c("that","the","and","not","when","was","with"))
#Building the term-document matric
TextDoc_tdm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_tdm)
#Sort by decreasing value of Freq
dtm_v <- sort(rowSums(dtm_m), decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v), freq=dtm_v)
#Display top 10
head(dtm_d, 10)
#generate word cloud
set.seed(1234)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 3,
max.words=100, random.order=FALSE, rot.per=0.40,
colors=brewer.pal(8, "Dark2"))
#Working with required columns for text analysis
dset_analysis <- dplyr::select(dset, c('regret'))
#Converting Dataframe into a Text Corpse
TextDoc <- Corpus(VectorSource(dset_analysis))
head(dset_analysis)
#Replacing : - and , with space
toSpace <- content_transformer(function(x , pattern) gsub(pattern," ", x))
TextDoc <- tm_map(TextDoc, toSpace, ",")
TextDoc <- tm_map(TextDoc, toSpace, "\"")
TextDoc <- tm_map(TextDoc, toSpace, "-")
#Convert all text to lowercase
TextDoc <- tm_map(TextDoc, content_transformer(tolower))
#Remove punctuations
TextDoc <- tm_map(TextDoc, removePunctuation)
#Eliminate extra white spaces
TextDoc <- tm_map(TextDoc, stripWhitespace)
TextDoc <- tm_map(TextDoc, removeWords, c("that","the","and","not","when","was","with"))
#Building the term-document matric
TextDoc_tdm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_tdm)
#Sort by decreasing value of Freq
dtm_v <- sort(rowSums(dtm_m), decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v), freq=dtm_v)
#Display top 10
head(dtm_d, 10)
#generate word cloud
set.seed(1234)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 3,
max.words=100, random.order=FALSE, rot.per=0.40,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.40,
colors=brewer.pal(8, "Dark2"))
#Performing emotion classification
dset <- get_nrc_sentiment(dset$regret)
head(dset,10)
nrow(dset)
td <- data.frame(t(dset))
#Computing column sums for each individual row and each level
td_new <- data.frame(rowSums(td[1:114]))
#Data cleaning and transformation
names(td_new)[1] <- "count"
td_new <- cbind("Sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2<- td_new[1:8,]
#Plot One - count of words associated with each sentiment
quickplot(Sentiment, data=td_new2, weight=count, geom="bar", fill=Sentiment, ylab="count")+ggtitle("Article sentiments")
#Performing emotion classification
dset <- get_nrc_sentiment(dset$regret)
head(dset,10)
nrow(dset)
td <- data.frame(t(dset))
#Computing column sums for each individual row and each level
td_new <- data.frame(rowSums(td[1:71]))
#Data cleaning and transformation
names(td_new)[1] <- "count"
td_new <- cbind("Sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2<- td_new[1:8,]
#Plot One - count of words associated with each sentiment
quickplot(Sentiment, data=td_new2, weight=count, geom="bar", fill=Sentiment, ylab="count")+ggtitle("Article sentiments")
#Plot two - count of words associated with each sentiment, expressed as a percentage
barplot(
sort(colSums(prop.table(dset[, 1:8]))),
horiz = TRUE,
cex.names = 0.7,
las = 1,
main = "Emotions in Text", xlab="Percentage"
)
#Performing emotion classification
dset_f <- get_nrc_sentiment(dset_f$regret)
head(dset_f,10)
nrow(dset_f)
td <- data.frame(t(dset_f))
#Computing column sums for each individual row and each level
td_new <- data.frame(rowSums(td[1:39]))
#Data cleaning and transformation
names(td_new)[1] <- "count"
td_new <- cbind("Sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2<- td_new[1:8,]
#Plot One - count of words associated with each sentiment
quickplot(Sentiment, data=td_new2, weight=count, geom="bar", fill=Sentiment, ylab="count")+ggtitle("Article sentiments")
#Plot two - count of words associated with each sentiment, expressed as a percentage
barplot(
sort(colSums(prop.table(dset[, 1:8]))),
horiz = TRUE,
cex.names = 0.7,
las = 1,
main = "Emotions in Text", xlab="Percentage"
)
#Performing emotion classification
dset_m <- get_nrc_sentiment(dset_m$regret)
head(dset_m,10)
nrow(dset_m)
td <- data.frame(t(dset_m))
#Computing column sums for each individual row and each level
td_new <- data.frame(rowSums(td[1:32]))
#Data cleaning and transformation
names(td_new)[1] <- "count"
td_new <- cbind("Sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2<- td_new[1:8,]
#Plot One - count of words associated with each sentiment
quickplot(Sentiment, data=td_new2, weight=count, geom="bar", fill=Sentiment, ylab="count")+ggtitle("Article sentiments")
#Plot two - count of words associated with each sentiment, expressed as a percentage
barplot(
sort(colSums(prop.table(dset[, 1:8]))),
horiz = TRUE,
cex.names = 0.7,
las = 1,
main = "Emotions in Text", xlab="Percentage"
)
#Plot One - count of words associated with each sentiment
quickplot(Sentiment, data=td_new2, weight=count, geom="bar", fill=Sentiment, ylab="count")+ggtitle("Article sentiments")
#Performing emotion classification
dset <- get_nrc_sentiment(dset$regret)
head(dset,10)
nrow(dset)
td <- data.frame(t(dset))
#Computing column sums for each individual row and each level
td_new <- data.frame(rowSums(td[1:71]))
#Data cleaning and transformation
names(td_new)[1] <- "count"
td_new <- cbind("Sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2<- td_new[1:8,]
#Plot One - count of words associated with each sentiment
quickplot(Sentiment, data=td_new2, weight=count, geom="bar", fill=Sentiment, ylab="count")+ggtitle("Article sentiments")
#Plot two - count of words associated with each sentiment, expressed as a percentage
barplot(
sort(colSums(prop.table(dset[, 1:8]))),
horiz = TRUE,
cex.names = 0.7,
las = 1,
main = "Emotions in Text", xlab="Percentage"
)
#Plot One - count of words associated with each sentiment
quickplot(Sentiment, data=td_new2, weight=count, geom="bar", fill=Sentiment, ylab="count")+ggtitle("Article sentiments")
